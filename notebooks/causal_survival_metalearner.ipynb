{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scenario_data(h5_file_path, scenario_num):\n",
    "    key = f\"scenario_{scenario_num}/data\"\n",
    "    with pd.HDFStore(h5_file_path, mode='r') as store:\n",
    "        if key not in store:\n",
    "            # print(f\"Scenario {scenario_num} not found in the HDF5 file.\")\n",
    "            return None  # Scenario not found\n",
    "        df = store[key]\n",
    "        metadata = store.get_storer(key).attrs.metadata\n",
    "    return {\"dataset\": df, \"metadata\": metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_files = [\n",
    "    \"../synthetic_data/RCT_0_5.h5\",\n",
    "    \"../synthetic_data/RCT_0_05.h5\",\n",
    "    \"../synthetic_data/e_X.h5\",\n",
    "    \"../synthetic_data/e_X_U.h5\",\n",
    "    \"../synthetic_data/e_X_no_overlap.h5\",\n",
    "    \"../synthetic_data/e_X_info_censor.h5\",\n",
    "    \"../synthetic_data/e_X_U_info_censor.h5\",\n",
    "    \"../synthetic_data/e_X_no_overlap_info_censor.h5\"\n",
    "]\n",
    "\n",
    "experiment_setups = {}\n",
    "\n",
    "for path in store_files:\n",
    "    base_name = os.path.splitext(os.path.basename(path))[0]  # e.g. RCT_0_5\n",
    "    scenario_dict = {}\n",
    "    for scenario in range(1, 11):\n",
    "        try:\n",
    "            result = load_scenario_data(path, scenario)\n",
    "            if result is not None:\n",
    "                scenario_dict[f\"scenario_{scenario}\"] = result\n",
    "        except Exception as e:\n",
    "            # Log or ignore as needed\n",
    "            print(f\"Error loading {path} scenario {scenario}: {e}\")\n",
    "            continue\n",
    "    experiment_setups[base_name] = scenario_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>observed_time</th>\n",
       "      <th>event</th>\n",
       "      <th>W</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>U1</th>\n",
       "      <th>U2</th>\n",
       "      <th>T0</th>\n",
       "      <th>T1</th>\n",
       "      <th>T</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.054267</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135488</td>\n",
       "      <td>0.887852</td>\n",
       "      <td>0.932606</td>\n",
       "      <td>0.445568</td>\n",
       "      <td>0.388236</td>\n",
       "      <td>0.151609</td>\n",
       "      <td>0.205535</td>\n",
       "      <td>0.054267</td>\n",
       "      <td>0.061394</td>\n",
       "      <td>0.054267</td>\n",
       "      <td>1.803019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.732630</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.257596</td>\n",
       "      <td>0.657368</td>\n",
       "      <td>0.492617</td>\n",
       "      <td>0.964238</td>\n",
       "      <td>0.800984</td>\n",
       "      <td>0.597208</td>\n",
       "      <td>0.255785</td>\n",
       "      <td>0.228566</td>\n",
       "      <td>0.732630</td>\n",
       "      <td>0.732630</td>\n",
       "      <td>1.689546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.162856</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.455205</td>\n",
       "      <td>0.801058</td>\n",
       "      <td>0.041718</td>\n",
       "      <td>0.769458</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.370382</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.176016</td>\n",
       "      <td>0.162856</td>\n",
       "      <td>0.162856</td>\n",
       "      <td>1.256329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.050340</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.292809</td>\n",
       "      <td>0.610914</td>\n",
       "      <td>0.913027</td>\n",
       "      <td>0.300115</td>\n",
       "      <td>0.248599</td>\n",
       "      <td>0.038464</td>\n",
       "      <td>0.409829</td>\n",
       "      <td>0.381909</td>\n",
       "      <td>0.050340</td>\n",
       "      <td>0.050340</td>\n",
       "      <td>1.241777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.524607</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666392</td>\n",
       "      <td>0.987533</td>\n",
       "      <td>0.468270</td>\n",
       "      <td>0.123287</td>\n",
       "      <td>0.916031</td>\n",
       "      <td>0.342961</td>\n",
       "      <td>0.791330</td>\n",
       "      <td>0.524607</td>\n",
       "      <td>1.121968</td>\n",
       "      <td>0.524607</td>\n",
       "      <td>1.516613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  observed_time  event  W        X1        X2        X3        X4  \\\n",
       "0   0       0.054267      1  0  0.135488  0.887852  0.932606  0.445568   \n",
       "1   1       0.732630      1  1  0.257596  0.657368  0.492617  0.964238   \n",
       "2   2       0.162856      1  1  0.455205  0.801058  0.041718  0.769458   \n",
       "3   3       0.050340      1  1  0.292809  0.610914  0.913027  0.300115   \n",
       "4   4       0.524607      1  0  0.666392  0.987533  0.468270  0.123287   \n",
       "\n",
       "         X5        U1        U2        T0        T1         T         C  \n",
       "0  0.388236  0.151609  0.205535  0.054267  0.061394  0.054267  1.803019  \n",
       "1  0.800984  0.597208  0.255785  0.228566  0.732630  0.732630  1.689546  \n",
       "2  0.003171  0.370382  0.223214  0.176016  0.162856  0.162856  1.256329  \n",
       "3  0.248599  0.038464  0.409829  0.381909  0.050340  0.050340  1.241777  \n",
       "4  0.916031  0.342961  0.791330  0.524607  1.121968  0.524607  1.516613  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_setups['RCT_0_5']['scenario_1']['dataset'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_idx0</th>\n",
       "      <th>random_idx1</th>\n",
       "      <th>random_idx2</th>\n",
       "      <th>random_idx3</th>\n",
       "      <th>random_idx4</th>\n",
       "      <th>random_idx5</th>\n",
       "      <th>random_idx6</th>\n",
       "      <th>random_idx7</th>\n",
       "      <th>random_idx8</th>\n",
       "      <th>random_idx9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47390</td>\n",
       "      <td>5618</td>\n",
       "      <td>14210</td>\n",
       "      <td>46970</td>\n",
       "      <td>4203</td>\n",
       "      <td>16369</td>\n",
       "      <td>24535</td>\n",
       "      <td>45204</td>\n",
       "      <td>45725</td>\n",
       "      <td>45885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38566</td>\n",
       "      <td>46218</td>\n",
       "      <td>39045</td>\n",
       "      <td>7253</td>\n",
       "      <td>22759</td>\n",
       "      <td>34401</td>\n",
       "      <td>28889</td>\n",
       "      <td>38471</td>\n",
       "      <td>45822</td>\n",
       "      <td>37471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32814</td>\n",
       "      <td>20226</td>\n",
       "      <td>40012</td>\n",
       "      <td>4854</td>\n",
       "      <td>27351</td>\n",
       "      <td>39165</td>\n",
       "      <td>25359</td>\n",
       "      <td>14516</td>\n",
       "      <td>25717</td>\n",
       "      <td>29860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41393</td>\n",
       "      <td>39492</td>\n",
       "      <td>27153</td>\n",
       "      <td>19041</td>\n",
       "      <td>33009</td>\n",
       "      <td>19822</td>\n",
       "      <td>21243</td>\n",
       "      <td>41228</td>\n",
       "      <td>955</td>\n",
       "      <td>23901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12564</td>\n",
       "      <td>17823</td>\n",
       "      <td>48976</td>\n",
       "      <td>18458</td>\n",
       "      <td>22756</td>\n",
       "      <td>28169</td>\n",
       "      <td>45851</td>\n",
       "      <td>36620</td>\n",
       "      <td>29824</td>\n",
       "      <td>12711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>15948</td>\n",
       "      <td>39245</td>\n",
       "      <td>30779</td>\n",
       "      <td>48178</td>\n",
       "      <td>45056</td>\n",
       "      <td>4892</td>\n",
       "      <td>528</td>\n",
       "      <td>7486</td>\n",
       "      <td>31042</td>\n",
       "      <td>38267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>11102</td>\n",
       "      <td>29624</td>\n",
       "      <td>40779</td>\n",
       "      <td>3136</td>\n",
       "      <td>45904</td>\n",
       "      <td>41903</td>\n",
       "      <td>45682</td>\n",
       "      <td>36621</td>\n",
       "      <td>33204</td>\n",
       "      <td>38070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>16338</td>\n",
       "      <td>8986</td>\n",
       "      <td>19293</td>\n",
       "      <td>35651</td>\n",
       "      <td>10172</td>\n",
       "      <td>17947</td>\n",
       "      <td>38843</td>\n",
       "      <td>18310</td>\n",
       "      <td>2765</td>\n",
       "      <td>12581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>32478</td>\n",
       "      <td>32134</td>\n",
       "      <td>11955</td>\n",
       "      <td>36939</td>\n",
       "      <td>33266</td>\n",
       "      <td>41932</td>\n",
       "      <td>43910</td>\n",
       "      <td>21691</td>\n",
       "      <td>40801</td>\n",
       "      <td>33527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>23102</td>\n",
       "      <td>2305</td>\n",
       "      <td>33545</td>\n",
       "      <td>20070</td>\n",
       "      <td>20726</td>\n",
       "      <td>1875</td>\n",
       "      <td>26988</td>\n",
       "      <td>24790</td>\n",
       "      <td>17795</td>\n",
       "      <td>39733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       random_idx0  random_idx1  random_idx2  random_idx3  random_idx4  \\\n",
       "idx                                                                      \n",
       "0            47390         5618        14210        46970         4203   \n",
       "1            38566        46218        39045         7253        22759   \n",
       "2            32814        20226        40012         4854        27351   \n",
       "3            41393        39492        27153        19041        33009   \n",
       "4            12564        17823        48976        18458        22756   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "49995        15948        39245        30779        48178        45056   \n",
       "49996        11102        29624        40779         3136        45904   \n",
       "49997        16338         8986        19293        35651        10172   \n",
       "49998        32478        32134        11955        36939        33266   \n",
       "49999        23102         2305        33545        20070        20726   \n",
       "\n",
       "       random_idx5  random_idx6  random_idx7  random_idx8  random_idx9  \n",
       "idx                                                                     \n",
       "0            16369        24535        45204        45725        45885  \n",
       "1            34401        28889        38471        45822        37471  \n",
       "2            39165        25359        14516        25717        29860  \n",
       "3            19822        21243        41228          955        23901  \n",
       "4            28169        45851        36620        29824        12711  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "49995         4892          528         7486        31042        38267  \n",
       "49996        41903        45682        36621        33204        38070  \n",
       "49997        17947        38843        18310         2765        12581  \n",
       "49998        41932        43910        21691        40801        33527  \n",
       "49999         1875        26988        24790        17795        39733  \n",
       "\n",
       "[50000 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_repeat_setups = pd.read_csv(\"../synthetic_data/idx_split.csv\").set_index(\"idx\")\n",
    "experiment_repeat_setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_repeats_to_include = 2  # max 10\n",
    "num_training_data_points = 200 # max 45000\n",
    "test_size = 5000\n",
    "survival_model = 'DeepHit' # \n",
    "meta_learner_type = 't_learner_survival'\n",
    "# load_imputed_values = True\n",
    "# imputed_times_path = f\"synthetic_data/imputed_times_lookup.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pickle_path = f\"results/{meta_learner_type}_{survival_model}_num_repeats_{num_repeats_to_include}_train_size_{num_training_data_points}.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models_causal_impute.meta_learners import TLearner, SLearner, XLearner\n",
    "# from models_causal_impute.survival_eval_impute import SurvivalEvalImputer\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of \"notebooks\" to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from models_causal_survival_meta.meta_learners_survival import TLearnerSurvival, SLearnerSurvival, MatchingLearnerSurvival\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_split(dataset_df, experiment_repeat_setups, random_idx_col_list, num_training_data_points=5000, test_size=5000):\n",
    "    split_results = {}\n",
    "\n",
    "    for rand_idx in random_idx_col_list:\n",
    "        random_idx = experiment_repeat_setups[rand_idx].values\n",
    "        test_ids = random_idx[-test_size:]\n",
    "        train_ids = random_idx[:min(num_training_data_points, len(random_idx) - test_size)]\n",
    "\n",
    "        X_cols = [c for c in dataset_df.columns if c.startswith(\"X\") and c[1:].isdigit()]\n",
    "        \n",
    "        train_df = dataset_df[dataset_df['id'].isin(train_ids)]\n",
    "        test_df = dataset_df[dataset_df['id'].isin(test_ids)]\n",
    "\n",
    "        X_train = train_df[X_cols].to_numpy()\n",
    "        W_train = train_df[\"W\"].to_numpy()\n",
    "        Y_train = train_df[[\"observed_time\", \"event\"]].to_numpy()\n",
    "\n",
    "        X_test = test_df[X_cols].to_numpy()\n",
    "        W_test = test_df[\"W\"].to_numpy()\n",
    "        Y_test = test_df[[\"observed_time\", \"event\"]].to_numpy()\n",
    "\n",
    "        cate_test_true = (test_df[\"T1\"] - test_df[\"T0\"]).to_numpy()\n",
    "\n",
    "        split_results[rand_idx] = (X_train, W_train, Y_train, X_test, W_test, Y_test, cate_test_true)\n",
    "\n",
    "    return split_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output results path: results/t_learner_survival_DeepHit_mean_repeats_2_train_200.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Setups:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calculating integrated brier score: all times must be within follow-up time of test data: [0.009524181732724112; 1.2595473527908325[\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calculating integrated brier score: all times must be within follow-up time of test data: [1.0; 11.0[\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calculating integrated brier score: all times must be within follow-up time of test data: [1.0; 13.0[\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calculating integrated brier score: all times must be within follow-up time of test data: [0.0; 8.0[\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RCT_0_5 Scenarios: 100%|██████████| 5/5 [01:54<00:00, 22.98s/it]\n",
      "Experiment Setups:   0%|          | 0/8 [01:54<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "experiment_repeat_setups = pd.read_csv(\"synthetic_data/idx_split.csv\").set_index(\"idx\")\n",
    "random_idx_col_list = experiment_repeat_setups.columns.to_list()[:num_repeats_to_include]\n",
    "\n",
    "output_pickle_path = f\"results/{meta_learner_type}_{survival_model}_{'mean'}_repeats_{num_repeats_to_include}_train_{num_training_data_points}.pkl\"\n",
    "print(\"Output results path:\", output_pickle_path)\n",
    "\n",
    "# Define base survival models to use\n",
    "base_model = survival_model\n",
    "results_dict = {}\n",
    "\n",
    "# Define hyperparameter grids for each model\n",
    "hyperparameter_grids = {\n",
    "    'RandomSurvivalForest': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'min_samples_split': [5, 10],\n",
    "        'min_samples_leaf': [3, 5]\n",
    "    },\n",
    "    'DeepSurv': {\n",
    "        'num_nodes': [32, 64],\n",
    "        'dropout': [0.1, 0.2],\n",
    "        'lr': [0.01, 0.001],\n",
    "        'epochs': [100, 500]\n",
    "    },\n",
    "    'DeepHit': {\n",
    "        'num_nodes': [32, 64],\n",
    "        'dropout': [0.1, 0.2],\n",
    "        'lr': [0.01, 0.001],\n",
    "        'epochs': [100, 500]\n",
    "    }\n",
    "}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for setup_name, setup_dict in tqdm(experiment_setups.items(), desc=\"Experiment Setups\"):\n",
    "    results_dict[setup_name] = {}\n",
    "    for scenario_key in tqdm(setup_dict, desc=f\"{setup_name} Scenarios\"):\n",
    "\n",
    "        # counter += 1\n",
    "        # if counter < 5:\n",
    "        #     continue\n",
    "\n",
    "        dataset_df = setup_dict[scenario_key][\"dataset\"]\n",
    "        split_dict = prepare_data_split(dataset_df, experiment_repeat_setups, random_idx_col_list, num_training_data_points)\n",
    "        results_dict[setup_name][scenario_key] = {}\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for rand_idx in random_idx_col_list:\n",
    "            X_train, W_train, Y_train, X_test, W_test, Y_test, cate_test_true = split_dict[rand_idx]\n",
    "\n",
    "            max_time = Y_train[:, 0].max()\n",
    "            \n",
    "            # Initialize the appropriate meta-learner\n",
    "            if meta_learner_type == \"t_learner_survival\":\n",
    "                learner = TLearnerSurvival(\n",
    "                    base_model_name=base_model,\n",
    "                    base_model_grid=hyperparameter_grids,\n",
    "                    metric=\"mean\",\n",
    "                    max_time=max_time\n",
    "                )\n",
    "            elif meta_learner_type == \"s_learner_survival\":\n",
    "                learner = SLearnerSurvival(\n",
    "                    base_model_name=base_model,\n",
    "                    base_model_grid=hyperparameter_grids,\n",
    "                    metric=\"mean\",\n",
    "                    max_time=max_time\n",
    "                )\n",
    "            elif meta_learner_type == \"matching_survival\":\n",
    "                learner = MatchingLearnerSurvival(\n",
    "                    base_model_name=base_model,\n",
    "                    base_model_grid=hyperparameter_grids,\n",
    "                    metric=\"mean\",\n",
    "                    num_matches=5,\n",
    "                    max_time=max_time\n",
    "                )\n",
    "\n",
    "            # Fit the learner\n",
    "            learner.fit(X_train, W_train, Y_train)\n",
    "            \n",
    "            # Evaluate base survival models on test data\n",
    "            base_model_eval = learner.evaluate_test(X_test, Y_test, W_test)\n",
    "            \n",
    "            # Evaluate causal effect predictions\n",
    "            mse_test, cate_test_pred, ate_test_pred = learner.evaluate(X_test, cate_test_true, W_test)\n",
    "\n",
    "            results_dict[setup_name][scenario_key][rand_idx] = {\n",
    "                \"cate_true\": cate_test_true,\n",
    "                \"cate_pred\": cate_test_pred,\n",
    "                \"ate_true\": cate_test_true.mean(),\n",
    "                \"ate_pred\": ate_test_pred,\n",
    "                \"cate_mse\": mse_test,\n",
    "                \"ate_bias\": ate_test_pred - cate_test_true.mean(),\n",
    "                \"base_model_eval\": base_model_eval  # Store base model evaluation results\n",
    "            }\n",
    "\n",
    "        end_time = time.time()\n",
    "        avg = results_dict[setup_name][scenario_key]\n",
    "        results_dict[setup_name][scenario_key][\"average\"] = {\n",
    "            \"mean_cate_mse\": np.mean([avg[i][\"cate_mse\"] for i in random_idx_col_list]),\n",
    "            \"std_cate_mse\": np.std([avg[i][\"cate_mse\"] for i in random_idx_col_list]),\n",
    "            \"mean_ate_pred\": np.mean([avg[i][\"ate_pred\"] for i in random_idx_col_list]),\n",
    "            \"std_ate_pred\": np.std([avg[i][\"ate_pred\"] for i in random_idx_col_list]),\n",
    "            \"mean_ate_true\": np.mean([avg[i][\"ate_true\"] for i in random_idx_col_list]),\n",
    "            \"std_ate_true\": np.std([avg[i][\"ate_true\"] for i in random_idx_col_list]),\n",
    "            \"mean_ate_bias\": np.mean([avg[i][\"ate_bias\"] for i in random_idx_col_list]),\n",
    "            \"std_ate_bias\": np.std([avg[i][\"ate_bias\"] for i in random_idx_col_list]),\n",
    "            \"runtime\": (end_time - start_time) / len(random_idx_col_list),\n",
    "            \"base_model_eval\" : {\n",
    "                                    base_model_k: {\n",
    "                                        f\"{stat}_{metric_j}\": func([\n",
    "                                            avg[i]['base_model_eval'][base_model_k][metric_j] for i in random_idx_col_list\n",
    "                                        ])\n",
    "                                        for metric_j in metric_j_dict\n",
    "                                        for stat, func in zip(['mean', 'std'], [np.nanmean, np.nanstd])\n",
    "                                    }\n",
    "                                    for base_model_k, metric_j_dict in avg[random_idx_col_list[0]]['base_model_eval'].items()\n",
    "                                }\n",
    "            }\n",
    "    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_survival_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
