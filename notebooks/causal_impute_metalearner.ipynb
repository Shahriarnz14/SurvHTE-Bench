{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scenario_data(h5_file_path, scenario_num):\n",
    "    key = f\"scenario_{scenario_num}/data\"\n",
    "    with pd.HDFStore(h5_file_path, mode='r') as store:\n",
    "        if key not in store:\n",
    "            return None  # Scenario not found\n",
    "        df = store[key]\n",
    "        metadata = store.get_storer(key).attrs.metadata\n",
    "    return {\"dataset\": df, \"metadata\": metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_files = [\n",
    "    \"../synthetic_data/RCT_0_5.h5\",\n",
    "    \"../synthetic_data/RCT_0_05.h5\",\n",
    "    \"../synthetic_data/e_X.h5\",\n",
    "    \"../synthetic_data/e_X_U.h5\",\n",
    "    \"../synthetic_data/e_X_no_overlap.h5\",\n",
    "    \"../synthetic_data/e_X_info_censor.h5\",\n",
    "    \"../synthetic_data/e_X_U_info_censor.h5\",\n",
    "    \"../synthetic_data/e_X_no_overlap_info_censor.h5\"\n",
    "]\n",
    "\n",
    "experiment_setups = {}\n",
    "\n",
    "for path in store_files:\n",
    "    base_name = os.path.splitext(os.path.basename(path))[0]  # e.g. RCT_0_5\n",
    "    scenario_dict = {}\n",
    "    for scenario in range(1, 11):\n",
    "        try:\n",
    "            result = load_scenario_data(path, scenario)\n",
    "            if result is not None:\n",
    "                scenario_dict[f\"scenario_{scenario}\"] = result\n",
    "        except Exception as e:\n",
    "            # Log or ignore as needed\n",
    "            continue\n",
    "    experiment_setups[base_name] = scenario_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>observed_time</th>\n",
       "      <th>event</th>\n",
       "      <th>W</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>U1</th>\n",
       "      <th>U2</th>\n",
       "      <th>T0</th>\n",
       "      <th>T1</th>\n",
       "      <th>T</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.054267</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135488</td>\n",
       "      <td>0.887852</td>\n",
       "      <td>0.932606</td>\n",
       "      <td>0.445568</td>\n",
       "      <td>0.388236</td>\n",
       "      <td>0.151609</td>\n",
       "      <td>0.205535</td>\n",
       "      <td>0.054267</td>\n",
       "      <td>0.061394</td>\n",
       "      <td>0.054267</td>\n",
       "      <td>1.803019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.732630</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.257596</td>\n",
       "      <td>0.657368</td>\n",
       "      <td>0.492617</td>\n",
       "      <td>0.964238</td>\n",
       "      <td>0.800984</td>\n",
       "      <td>0.597208</td>\n",
       "      <td>0.255785</td>\n",
       "      <td>0.228566</td>\n",
       "      <td>0.732630</td>\n",
       "      <td>0.732630</td>\n",
       "      <td>1.689546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.162856</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.455205</td>\n",
       "      <td>0.801058</td>\n",
       "      <td>0.041718</td>\n",
       "      <td>0.769458</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.370382</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.176016</td>\n",
       "      <td>0.162856</td>\n",
       "      <td>0.162856</td>\n",
       "      <td>1.256329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.050340</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.292809</td>\n",
       "      <td>0.610914</td>\n",
       "      <td>0.913027</td>\n",
       "      <td>0.300115</td>\n",
       "      <td>0.248599</td>\n",
       "      <td>0.038464</td>\n",
       "      <td>0.409829</td>\n",
       "      <td>0.381909</td>\n",
       "      <td>0.050340</td>\n",
       "      <td>0.050340</td>\n",
       "      <td>1.241777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.524607</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666392</td>\n",
       "      <td>0.987533</td>\n",
       "      <td>0.468270</td>\n",
       "      <td>0.123287</td>\n",
       "      <td>0.916031</td>\n",
       "      <td>0.342961</td>\n",
       "      <td>0.791330</td>\n",
       "      <td>0.524607</td>\n",
       "      <td>1.121968</td>\n",
       "      <td>0.524607</td>\n",
       "      <td>1.516613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  observed_time  event  W        X1        X2        X3        X4  \\\n",
       "0   0       0.054267      1  0  0.135488  0.887852  0.932606  0.445568   \n",
       "1   1       0.732630      1  1  0.257596  0.657368  0.492617  0.964238   \n",
       "2   2       0.162856      1  1  0.455205  0.801058  0.041718  0.769458   \n",
       "3   3       0.050340      1  1  0.292809  0.610914  0.913027  0.300115   \n",
       "4   4       0.524607      1  0  0.666392  0.987533  0.468270  0.123287   \n",
       "\n",
       "         X5        U1        U2        T0        T1         T         C  \n",
       "0  0.388236  0.151609  0.205535  0.054267  0.061394  0.054267  1.803019  \n",
       "1  0.800984  0.597208  0.255785  0.228566  0.732630  0.732630  1.689546  \n",
       "2  0.003171  0.370382  0.223214  0.176016  0.162856  0.162856  1.256329  \n",
       "3  0.248599  0.038464  0.409829  0.381909  0.050340  0.050340  1.241777  \n",
       "4  0.916031  0.342961  0.791330  0.524607  1.121968  0.524607  1.516613  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_setups['RCT_0_5']['scenario_1']['dataset'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_idx0</th>\n",
       "      <th>random_idx1</th>\n",
       "      <th>random_idx2</th>\n",
       "      <th>random_idx3</th>\n",
       "      <th>random_idx4</th>\n",
       "      <th>random_idx5</th>\n",
       "      <th>random_idx6</th>\n",
       "      <th>random_idx7</th>\n",
       "      <th>random_idx8</th>\n",
       "      <th>random_idx9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47390</td>\n",
       "      <td>5618</td>\n",
       "      <td>14210</td>\n",
       "      <td>46970</td>\n",
       "      <td>4203</td>\n",
       "      <td>16369</td>\n",
       "      <td>24535</td>\n",
       "      <td>45204</td>\n",
       "      <td>45725</td>\n",
       "      <td>45885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38566</td>\n",
       "      <td>46218</td>\n",
       "      <td>39045</td>\n",
       "      <td>7253</td>\n",
       "      <td>22759</td>\n",
       "      <td>34401</td>\n",
       "      <td>28889</td>\n",
       "      <td>38471</td>\n",
       "      <td>45822</td>\n",
       "      <td>37471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32814</td>\n",
       "      <td>20226</td>\n",
       "      <td>40012</td>\n",
       "      <td>4854</td>\n",
       "      <td>27351</td>\n",
       "      <td>39165</td>\n",
       "      <td>25359</td>\n",
       "      <td>14516</td>\n",
       "      <td>25717</td>\n",
       "      <td>29860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41393</td>\n",
       "      <td>39492</td>\n",
       "      <td>27153</td>\n",
       "      <td>19041</td>\n",
       "      <td>33009</td>\n",
       "      <td>19822</td>\n",
       "      <td>21243</td>\n",
       "      <td>41228</td>\n",
       "      <td>955</td>\n",
       "      <td>23901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12564</td>\n",
       "      <td>17823</td>\n",
       "      <td>48976</td>\n",
       "      <td>18458</td>\n",
       "      <td>22756</td>\n",
       "      <td>28169</td>\n",
       "      <td>45851</td>\n",
       "      <td>36620</td>\n",
       "      <td>29824</td>\n",
       "      <td>12711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>15948</td>\n",
       "      <td>39245</td>\n",
       "      <td>30779</td>\n",
       "      <td>48178</td>\n",
       "      <td>45056</td>\n",
       "      <td>4892</td>\n",
       "      <td>528</td>\n",
       "      <td>7486</td>\n",
       "      <td>31042</td>\n",
       "      <td>38267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>11102</td>\n",
       "      <td>29624</td>\n",
       "      <td>40779</td>\n",
       "      <td>3136</td>\n",
       "      <td>45904</td>\n",
       "      <td>41903</td>\n",
       "      <td>45682</td>\n",
       "      <td>36621</td>\n",
       "      <td>33204</td>\n",
       "      <td>38070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>16338</td>\n",
       "      <td>8986</td>\n",
       "      <td>19293</td>\n",
       "      <td>35651</td>\n",
       "      <td>10172</td>\n",
       "      <td>17947</td>\n",
       "      <td>38843</td>\n",
       "      <td>18310</td>\n",
       "      <td>2765</td>\n",
       "      <td>12581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>32478</td>\n",
       "      <td>32134</td>\n",
       "      <td>11955</td>\n",
       "      <td>36939</td>\n",
       "      <td>33266</td>\n",
       "      <td>41932</td>\n",
       "      <td>43910</td>\n",
       "      <td>21691</td>\n",
       "      <td>40801</td>\n",
       "      <td>33527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>23102</td>\n",
       "      <td>2305</td>\n",
       "      <td>33545</td>\n",
       "      <td>20070</td>\n",
       "      <td>20726</td>\n",
       "      <td>1875</td>\n",
       "      <td>26988</td>\n",
       "      <td>24790</td>\n",
       "      <td>17795</td>\n",
       "      <td>39733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       random_idx0  random_idx1  random_idx2  random_idx3  random_idx4  \\\n",
       "idx                                                                      \n",
       "0            47390         5618        14210        46970         4203   \n",
       "1            38566        46218        39045         7253        22759   \n",
       "2            32814        20226        40012         4854        27351   \n",
       "3            41393        39492        27153        19041        33009   \n",
       "4            12564        17823        48976        18458        22756   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "49995        15948        39245        30779        48178        45056   \n",
       "49996        11102        29624        40779         3136        45904   \n",
       "49997        16338         8986        19293        35651        10172   \n",
       "49998        32478        32134        11955        36939        33266   \n",
       "49999        23102         2305        33545        20070        20726   \n",
       "\n",
       "       random_idx5  random_idx6  random_idx7  random_idx8  random_idx9  \n",
       "idx                                                                     \n",
       "0            16369        24535        45204        45725        45885  \n",
       "1            34401        28889        38471        45822        37471  \n",
       "2            39165        25359        14516        25717        29860  \n",
       "3            19822        21243        41228          955        23901  \n",
       "4            28169        45851        36620        29824        12711  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "49995         4892          528         7486        31042        38267  \n",
       "49996        41903        45682        36621        33204        38070  \n",
       "49997        17947        38843        18310         2765        12581  \n",
       "49998        41932        43910        21691        40801        33527  \n",
       "49999         1875        26988        24790        17795        39733  \n",
       "\n",
       "[50000 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_repeat_setups = pd.read_csv(\"../synthetic_data/idx_split.csv\").set_index(\"idx\")\n",
    "experiment_repeat_setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_repeats_to_include = 10  # max 10\n",
    "num_training_data_points = 500 # max 45000\n",
    "test_size = 5000\n",
    "imputation_method = 'Pseudo_obs' # 'Pseudo_obs', 'Margin', 'IPCW-T'\n",
    "meta_learner_type = 'x_learner'\n",
    "load_imputed_values = True\n",
    "imputed_times_path = f\"../synthetic_data/imputed_times_lookup.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pickle_path = f\"results/{meta_learner_type}_{imputation_method}_num_repeats_{num_repeats_to_include}_train_size_{num_training_data_points}.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of \"notebooks\" to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from models_causal_impute_meta.meta_learners import TLearner, SLearner, XLearner\n",
    "from models_causal_impute_meta.survival_eval_impute import SurvivalEvalImputer\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_split(dataset_df, experiment_repeat_setups, random_idx_col_list, num_training_data_points=5000, test_size=5000):\n",
    "    split_results = {}\n",
    "\n",
    "    for rand_idx in random_idx_col_list:\n",
    "        random_idx = experiment_repeat_setups[rand_idx].values\n",
    "        test_ids = random_idx[-test_size:]\n",
    "        train_ids = random_idx[:min(num_training_data_points, len(random_idx) - test_size)]\n",
    "\n",
    "        X_cols = [c for c in dataset_df.columns if c.startswith(\"X\") and c[1:].isdigit()]\n",
    "        \n",
    "        train_df = dataset_df[dataset_df['id'].isin(train_ids)]\n",
    "        test_df = dataset_df[dataset_df['id'].isin(test_ids)]\n",
    "\n",
    "        X_train = train_df[X_cols].to_numpy()\n",
    "        W_train = train_df[\"W\"].to_numpy()\n",
    "        Y_train = train_df[[\"observed_time\", \"event\"]].to_numpy()\n",
    "\n",
    "        X_test = test_df[X_cols].to_numpy()\n",
    "        W_test = test_df[\"W\"].to_numpy()\n",
    "        Y_test = test_df[[\"observed_time\", \"event\"]].to_numpy()\n",
    "\n",
    "        cate_test_true = (test_df[\"T1\"] - test_df[\"T0\"]).to_numpy()\n",
    "\n",
    "        split_results[rand_idx] = (X_train, W_train, Y_train, X_test, W_test, Y_test, cate_test_true)\n",
    "\n",
    "    return split_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output results path: results/x_learner_Pseudo_obs_num_repeats_10_train_size_500.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Setups:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "RCT_0_5 Scenarios:   0%|          | 0/5 [00:06<?, ?it/s]\n",
      "Experiment Setups:   0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "random_idx_col_list = experiment_repeat_setups.columns.to_list()[:num_repeats_to_include]\n",
    "\n",
    "print(\"Output results path:\", output_pickle_path)\n",
    "\n",
    "base_regressors = ['ridge', 'lasso', 'rf', 'gbr', 'xgb']\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for setup_name, setup_dict in tqdm(experiment_setups.items(), desc=\"Experiment Setups\"):\n",
    "\n",
    "    results_dict[setup_name] = {}\n",
    "\n",
    "    for scenario_key in tqdm(setup_dict, desc=f\"{setup_name} Scenarios\"):\n",
    "        dataset_df = setup_dict[scenario_key][\"dataset\"]\n",
    "        split_dict = prepare_data_split(dataset_df, experiment_repeat_setups, random_idx_col_list, num_training_data_points, test_size)\n",
    "\n",
    "        # Initialize results dictionary for this setup and scenario\n",
    "        results_dict[setup_name][scenario_key] = {}\n",
    "\n",
    "        # For each base model, we will run the TLearner\n",
    "        for base_model in tqdm(base_regressors, desc=\"Base Models\", leave=False):\n",
    "            # print(f\"Running {base_model} for {setup_name} - {scenario_key}\")\n",
    "            \n",
    "            # Store placeholder for later population\n",
    "            results_dict[setup_name][scenario_key][base_model] = {}\n",
    "            \n",
    "            start_time = time.time()\n",
    "\n",
    "            for rand_idx in random_idx_col_list:\n",
    "                X_train, W_train, Y_train, X_test, W_test, Y_test, cate_test_true = split_dict[rand_idx]\n",
    "\n",
    "                Y_train_imputed, Y_test_imputed = None, None\n",
    "                if load_imputed_values:\n",
    "                    # Load imputed values\n",
    "                    with open(imputed_times_path, \"rb\") as f:\n",
    "                        imputed_times = pickle.load(f)\n",
    "\n",
    "                    # Get imputed values for the current random index\n",
    "                    imputed_results = imputed_times.get(imputation_method, {}).get(setup_name, {}).get(scenario_key, {}).get(num_training_data_points, {}).get(rand_idx, {})\n",
    "                    Y_train_imputed = imputed_results.get(\"Y_train_imputed\", None)\n",
    "                    Y_test_imputed = imputed_results.get(\"Y_test_imputed\", None)\n",
    "                \n",
    "                # If imputed values are not loaded, use the original Y_train and Y_test\n",
    "                if Y_train_imputed is None:\n",
    "                    # impute the missing values\n",
    "                    print(f\"[Train] Imputing missing values for {setup_name} - {scenario_key} - {base_model} - {rand_idx}\")\n",
    "                    survival_imputer = SurvivalEvalImputer(imputation_method=imputation_method, verbose=False)\n",
    "                    Y_train_imputed, Y_test_imputed = survival_imputer.fit_transform(Y_train, Y_test)\n",
    "                \n",
    "                if Y_test_imputed is None:\n",
    "                    # impute the missing values\n",
    "                    print(f\"[Test] Imputing missing values for {setup_name} - {scenario_key} - {base_model} - {rand_idx}\")\n",
    "                    survival_imputer = SurvivalEvalImputer(imputation_method=imputation_method, verbose=False)\n",
    "                    _, Y_test_imputed = survival_imputer.fit_transform(Y_train, Y_test, impute_train=False)\n",
    "                \n",
    "                if meta_learner_type == 't_learner':\n",
    "                    learner = TLearner(base_model_name=base_model)\n",
    "                elif meta_learner_type == 's_learner':\n",
    "                    learner = SLearner(base_model_name=base_model)\n",
    "                elif meta_learner_type == 'x_learner':\n",
    "                    learner = XLearner(base_model_name=base_model)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown meta-learner type: {meta_learner_type}\")\n",
    "                \n",
    "                learner.fit(X_train, W_train, Y_train_imputed)\n",
    "                mse_test, cate_test_pred, ate_test_pred = learner.evaluate(X_test, cate_test_true, W_test)\n",
    "\n",
    "                # Evaluate base survival models on test data\n",
    "                base_model_eval = learner.evaluate_test(X_test, Y_test_imputed, W_test)\n",
    "\n",
    "                # Save results\n",
    "                results_dict[setup_name][scenario_key][base_model][rand_idx] = {\n",
    "                    \"cate_true\": cate_test_true,\n",
    "                    \"cate_pred\": cate_test_pred,\n",
    "                    \"ate_true\": cate_test_true.mean(),\n",
    "                    \"ate_pred\": ate_test_pred,\n",
    "                    \"cate_mse\": mse_test,\n",
    "                    \"ate_bias\": ate_test_pred - cate_test_true.mean(),\n",
    "                    \"base_model_eval\": base_model_eval, # Store base model evaluation results\n",
    "                }\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Save results to the setup dictionary\n",
    "            avg = results_dict[setup_name][scenario_key][base_model]\n",
    "            base_model_eval_performance = {\n",
    "                                            base_model_k: \n",
    "                                            {\n",
    "                                                f\"{stat}_{metric_j}\": func([\n",
    "                                                    avg[i]['base_model_eval'][base_model_k][metric_j] for i in random_idx_col_list\n",
    "                                                    if i in avg\n",
    "                                                ])\n",
    "                                                for metric_j in metric_j_dict\n",
    "                                                for stat, func in zip(['mean', 'std'], [np.nanmean, np.nanstd])\n",
    "                                            }\n",
    "                                            for base_model_k, metric_j_dict in avg[list(avg.keys())[0]]['base_model_eval'].items()\n",
    "                                        }\n",
    "\n",
    "            results_dict[setup_name][scenario_key][base_model][\"average\"] = {\n",
    "                    \"mean_cate_mse\": np.mean([avg[i][\"cate_mse\"] for i in random_idx_col_list]),\n",
    "                    \"std_cate_mse\": np.std([avg[i][\"cate_mse\"] for i in random_idx_col_list]),\n",
    "                    \"mean_ate_pred\": np.mean([avg[i][\"ate_pred\"] for i in random_idx_col_list]),\n",
    "                    \"std_ate_pred\": np.std([avg[i][\"ate_pred\"] for i in random_idx_col_list]),\n",
    "                    \"mean_ate_true\": np.mean([avg[i][\"ate_true\"] for i in random_idx_col_list]),\n",
    "                    \"std_ate_true\": np.std([avg[i][\"ate_true\"] for i in random_idx_col_list]),\n",
    "                    \"mean_ate_bias\": np.mean([avg[i][\"ate_bias\"] for i in random_idx_col_list]),\n",
    "                    \"std_ate_bias\": np.std([avg[i][\"ate_bias\"] for i in random_idx_col_list]),\n",
    "                    \"runtime\": (end_time - start_time) / len(random_idx_col_list),\n",
    "                    \"base_model_eval\": base_model_eval_performance\n",
    "                }\n",
    "\n",
    "            # Save progress to disk\n",
    "            # with open(output_pickle_path, \"wb\") as f:\n",
    "                # pickle.dump(results_dict, f)\n",
    "            \n",
    "            break\n",
    "        break\n",
    "    break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RCT_0_5': {'scenario_1': {'ridge': {'random_idx0': {'cate_true': array([ 0.01964673,  0.67548137,  0.01256466, ..., -0.08345062,\n",
       "             0.20826006, -0.22013693]),\n",
       "     'cate_pred': array([0.322864  , 0.05715146, 0.07463621, ..., 0.14558426, 0.26351616,\n",
       "            0.31013753]),\n",
       "     'ate_true': 0.12915102003284332,\n",
       "     'ate_pred': 0.1964462118366355,\n",
       "     'cate_mse': 0.6957057935431328,\n",
       "     'ate_bias': 0.06729519180379218,\n",
       "     'base_model_eval': {'mu1': {'mae': 0.4038342438314648,\n",
       "       'r2': 0.06901228503301671},\n",
       "      'mu0': {'mae': 0.2732396438908264, 'r2': 0.05553168254251639},\n",
       "      'tau0': {'mae': 0.2731728968341871, 'r2': 0.09351356163528213},\n",
       "      'tau1': {'mae': 0.4007390288386666, 'r2': 0.0009202576097605553},\n",
       "      'propensity': {'auc': 0.4871026212492847, 'f1': 0.5466595020598245}}},\n",
       "    'random_idx1': {'cate_true': array([ 0.04373459, -0.01348651, -0.12558549, ...,  2.88443699,\n",
       "            -0.03312028,  0.24602149]),\n",
       "     'cate_pred': array([ 0.0618785 , -0.00757634,  0.16259695, ...,  0.10593511,\n",
       "             0.02583559, -0.01900987]),\n",
       "     'ate_true': 0.12449698781077848,\n",
       "     'ate_pred': 0.04213053187055333,\n",
       "     'cate_mse': 0.7262979642006363,\n",
       "     'ate_bias': -0.08236645594022515,\n",
       "     'base_model_eval': {'mu1': {'mae': 0.33604761634484404,\n",
       "       'r2': 0.08159783128160325},\n",
       "      'mu0': {'mae': 0.2765757092551736, 'r2': 0.05760638445331412},\n",
       "      'tau0': {'mae': 0.27784711479683144, 'r2': 0.009672601794794788},\n",
       "      'tau1': {'mae': 0.3351846268737316, 'r2': 0.014469139289191912},\n",
       "      'propensity': {'auc': 0.5008317601330816, 'f1': 0.5425764192139738}}},\n",
       "    'random_idx2': {'cate_true': array([-0.13083455, -0.25443191,  0.22322109, ..., -0.04042443,\n",
       "             0.68333127, -0.12309126]),\n",
       "     'cate_pred': array([0.08335886, 0.17455641, 0.14838976, ..., 0.07151122, 0.2445476 ,\n",
       "            0.10095021]),\n",
       "     'ate_true': 0.11320535412052017,\n",
       "     'ate_pred': 0.15566401037152508,\n",
       "     'cate_mse': 0.5843032894265889,\n",
       "     'ate_bias': 0.04245865625100491,\n",
       "     'base_model_eval': {'mu1': {'mae': 0.35696954885178206,\n",
       "       'r2': 0.0970870142229554},\n",
       "      'mu0': {'mae': 0.26973962636043, 'r2': 0.030050273590835674},\n",
       "      'tau0': {'mae': 0.2726671430189974, 'r2': 0.004548491363134288},\n",
       "      'tau1': {'mae': 0.35897634520956095, 'r2': 0.02090732782751592},\n",
       "      'propensity': {'auc': 0.5100884145273169, 'f1': 0.5049685662137497}}},\n",
       "    'random_idx3': {'cate_true': array([-0.13083455, -0.29553616,  0.61790216, ..., -0.03312028,\n",
       "             1.12117897, -0.21827961]),\n",
       "     'cate_pred': array([ 0.02309727,  0.18033173,  0.26089682, ...,  0.18118638,\n",
       "             0.17266057, -0.00793415]),\n",
       "     'ate_true': 0.13422576870599365,\n",
       "     'ate_pred': 0.10078262172345667,\n",
       "     'cate_mse': 0.6969446498141421,\n",
       "     'ate_bias': -0.033443146982536975,\n",
       "     'base_model_eval': {'mu1': {'mae': 0.3235920522334324,\n",
       "       'r2': 0.08540845789665052},\n",
       "      'mu0': {'mae': 0.23672543661395104, 'r2': 0.05672049390703682},\n",
       "      'tau0': {'mae': 0.24005360164716302, 'r2': 0.0625894445566646},\n",
       "      'tau1': {'mae': 0.31974209125792874, 'r2': 0.021337379025387238},\n",
       "      'propensity': {'auc': 0.5072623166538086, 'f1': 0.16299704239237595}}},\n",
       "    'random_idx4': {'cate_true': array([ 0.0071266 ,  0.42334943,  0.11885308, ...,  1.40263868,\n",
       "            -0.44599299,  0.43901341]),\n",
       "     'cate_pred': array([-0.00495928,  0.07776867,  0.17294934, ...,  0.07445537,\n",
       "             0.19851351,  0.11216044]),\n",
       "     'ate_true': 0.1454709648166056,\n",
       "     'ate_pred': 0.09811944467350783,\n",
       "     'cate_mse': 0.8344096302630336,\n",
       "     'ate_bias': -0.047351520143097764,\n",
       "     'base_model_eval': {'mu1': {'mae': 0.385654657699436,\n",
       "       'r2': 0.08130701453354172},\n",
       "      'mu0': {'mae': 0.2852587677577432, 'r2': 0.030425295979329525},\n",
       "      'tau0': {'mae': 0.2845477713158111, 'r2': 0.0031185292640583118},\n",
       "      'tau1': {'mae': 0.38189602016045515, 'r2': 0.02209466152823969},\n",
       "      'propensity': {'auc': 0.48643913826108676, 'f1': 0.4971417307313227}}},\n",
       "    'random_idx5': {'cate_true': array([ 0.0071266 , -0.33156902, -0.13083455, ..., -0.08345062,\n",
       "            -0.22013693, -0.12309126]),\n",
       "     'cate_pred': array([ 0.05754616,  0.09865621,  0.01034528, ...,  0.11704777,\n",
       "             0.14935479, -0.00916134]),\n",
       "     'ate_true': 0.1150924596635593,\n",
       "     'ate_pred': 0.08384070759352892,\n",
       "     'cate_mse': 0.7270300934196472,\n",
       "     'ate_bias': -0.03125175207003038,\n",
       "     'base_model_eval': {'mu1': {'mae': 0.3236612541137408,\n",
       "       'r2': 0.08773981205977932},\n",
       "      'mu0': {'mae': 0.27127951419366214, 'r2': 0.01617547685535381},\n",
       "      'tau0': {'mae': 0.27136561534559117, 'r2': -0.0037647019931694725},\n",
       "      'tau1': {'mae': 0.32228796391367787, 'r2': 0.022475276006932665},\n",
       "      'propensity': {'auc': 0.5025036816023563, 'f1': 0.44169849472028755}}},\n",
       "    'random_idx6': {'cate_true': array([ 0.13581265,  0.01859975, -0.01528941, ...,  0.1718276 ,\n",
       "            -0.2025199 , -0.23328487]),\n",
       "     'cate_pred': array([0.00663943, 0.23425333, 0.10719548, ..., 0.07941986, 0.09975931,\n",
       "            0.08826139]),\n",
       "     'ate_true': 0.10842338016300965,\n",
       "     'ate_pred': 0.0951641703687382,\n",
       "     'cate_mse': 0.643205806266139,\n",
       "     'ate_bias': -0.013259209794271448,\n",
       "     'base_model_eval': {'mu1': {'mae': 0.3064243447371242,\n",
       "       'r2': 0.10994114281650924},\n",
       "      'mu0': {'mae': 0.2589926087401561, 'r2': 0.03398619782594825},\n",
       "      'tau0': {'mae': 0.2551560326986256, 'r2': -0.02353922675036113},\n",
       "      'tau1': {'mae': 0.3033988371091257, 'r2': 0.058601908218876786},\n",
       "      'propensity': {'auc': 0.49249980990799547, 'f1': 0.5448601525608427}}},\n",
       "    'random_idx7': {'cate_true': array([-0.09800848,  0.01964673,  0.11885308, ...,  1.12117897,\n",
       "            -0.08345062,  0.05523445]),\n",
       "     'cate_pred': array([ 0.19587006,  0.09752547, -0.0514979 , ...,  0.09668157,\n",
       "             0.08610461, -0.07942115]),\n",
       "     'ate_true': 0.13962732961458268,\n",
       "     'ate_pred': 0.060548801014019354,\n",
       "     'cate_mse': 0.6146430115480974,\n",
       "     'ate_bias': -0.07907852860056333,\n",
       "     'base_model_eval': {'mu1': {'mae': 0.35525500252401454,\n",
       "       'r2': 0.09275854770657277},\n",
       "      'mu0': {'mae': 0.27521058009984173, 'r2': 0.04523158442968689},\n",
       "      'tau0': {'mae': 0.27966006505029317, 'r2': 0.027236476806454468},\n",
       "      'tau1': {'mae': 0.3523140264031191, 'r2': 0.027432278631979168},\n",
       "      'propensity': {'auc': 0.4869477579116413, 'f1': 0.5052837573385519}}},\n",
       "    'random_idx8': {'cate_true': array([ 1.16187434,  0.14004801,  0.01956567, ..., -3.04577956,\n",
       "             0.05523445, -0.4338563 ]),\n",
       "     'cate_pred': array([ 0.02669928, -0.08476912, -0.08378973, ...,  0.23370987,\n",
       "            -0.00498726, -0.09442717]),\n",
       "     'ate_true': 0.12974416208389994,\n",
       "     'ate_pred': 0.03732208487940617,\n",
       "     'cate_mse': 0.6386432462033014,\n",
       "     'ate_bias': -0.09242207720449377,\n",
       "     'base_model_eval': {'mu1': {'mae': 0.3400438696412569,\n",
       "       'r2': 0.10113948655885097},\n",
       "      'mu0': {'mae': 0.297777593038208, 'r2': 0.026023055961420516},\n",
       "      'tau0': {'mae': 0.29489949142446564, 'r2': -0.014821483081178366},\n",
       "      'tau1': {'mae': 0.33672016623368045, 'r2': 0.04372125586637188},\n",
       "      'propensity': {'auc': 0.4973600925887992, 'f1': 0.6340670704683115}}},\n",
       "    'random_idx9': {'cate_true': array([ 0.03760702,  0.2885753 ,  1.12633537, ...,  0.71218822,\n",
       "             0.45455053, -0.09461645]),\n",
       "     'cate_pred': array([ 0.23074776,  0.2054681 , -0.02816467, ...,  0.15125062,\n",
       "             0.18354528, -0.00294009]),\n",
       "     'ate_true': 0.12530238395151086,\n",
       "     'ate_pred': 0.1209578691485942,\n",
       "     'cate_mse': 0.7158538656933469,\n",
       "     'ate_bias': -0.0043445148029166575,\n",
       "     'base_model_eval': {'mu1': {'mae': 0.3391680523065303,\n",
       "       'r2': 0.06562426326067006},\n",
       "      'mu0': {'mae': 0.26043628708495087, 'r2': 0.021854626003499966},\n",
       "      'tau0': {'mae': 0.2604806843053541, 'r2': 0.0010716929088474547},\n",
       "      'tau1': {'mae': 0.3386538405857251, 'r2': 0.0015257286054519703},\n",
       "      'propensity': {'auc': 0.49223470040514605, 'f1': 0.5936087295401403}}},\n",
       "    'average': {'mean_cate_mse': 0.6877037350378066,\n",
       "     'std_cate_mse': 0.06776213742446628,\n",
       "     'mean_ate_pred': 0.09909764534799652,\n",
       "     'std_ate_pred': 0.04685498278692181,\n",
       "     'mean_ate_true': 0.12647398109633037,\n",
       "     'std_ate_true': 0.011173842796182968,\n",
       "     'mean_ate_bias': -0.027376335748333834,\n",
       "     'std_ate_bias': 0.04994816697909686,\n",
       "     'runtime': 0.6574852228164673,\n",
       "     'base_model_eval': {'mu1': {'mean_mae': 0.3470650642283626,\n",
       "       'std_mae': 0.028036588971136762,\n",
       "       'mean_r2': 0.087161585537015,\n",
       "       'std_r2': 0.01306147997761773},\n",
       "      'mu0': {'mean_mae': 0.27052357670349425,\n",
       "       'std_mae': 0.0155051524312609,\n",
       "       'mean_r2': 0.037360507154894196,\n",
       "       'std_r2': 0.014517397831414137},\n",
       "      'tau0': {'mean_mae': 0.27098504164373194,\n",
       "       'std_mae': 0.014844550053092065,\n",
       "       'mean_r2': 0.015962538650452707,\n",
       "       'std_r2': 0.03430610781513235},\n",
       "      'tau1': {'mean_mae': 0.34499129465856715,\n",
       "       'std_mae': 0.027940319104424106,\n",
       "       'mean_r2': 0.023348521260970777,\n",
       "       'std_r2': 0.01656795170031679},\n",
       "      'propensity': {'mean_auc': 0.4963270293240517,\n",
       "       'std_auc': 0.008189582865473454,\n",
       "       'mean_f1': 0.497386146523938,\n",
       "       'std_f1': 0.12235251473618342}}}}}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_experiment_results(results_dict):\n",
    "    records = []\n",
    "\n",
    "    for setup_name, setup_dict in results_dict.items():\n",
    "        for scenario_key in setup_dict:\n",
    "            row = {\n",
    "                (\"setup_name\", \"\"): setup_name,\n",
    "                (\"scenario_key\", \"\"): scenario_key\n",
    "            }\n",
    "\n",
    "            for base_model in setup_dict[scenario_key]:\n",
    "                avg_result = setup_dict[scenario_key].get(base_model, {}).get(\"average\", {})\n",
    "                mean_mse = avg_result.get(\"mean_cate_mse\", np.nan)\n",
    "                std_mse = avg_result.get(\"std_cate_mse\", np.nan)\n",
    "                mean_ate_pred = avg_result.get(\"mean_ate_pred\", np.nan)\n",
    "                std_ate_pred = avg_result.get(\"std_ate_pred\", np.nan)\n",
    "                mean_ate_true = avg_result.get(\"mean_ate_true\", np.nan)\n",
    "                std_ate_true = avg_result.get(\"std_ate_true\", np.nan)\n",
    "                mean_ate_bias = avg_result.get(\"mean_ate_bias\", np.nan)\n",
    "                std_ate_bias = avg_result.get(\"std_ate_bias\", np.nan)\n",
    "                runtime = avg_result.get(\"runtime\", np.nan)\n",
    "\n",
    "                row[(base_model, \"CATE_MSE\")] = f\"{mean_mse:.3f} ± {std_mse:.3f}\" if not pd.isna(mean_mse) else np.nan\n",
    "                row[(base_model, \"ATE_pred\")] = f\"{mean_ate_pred:.3f} ± {std_ate_pred:.3f}\" if not pd.isna(mean_ate_pred) else np.nan\n",
    "                row[(base_model, \"ATE_true\")] = f\"{mean_ate_true:.3f} ± {std_ate_true:.3f}\" if not pd.isna(mean_ate_true) else np.nan\n",
    "                row[(base_model, \"ATE_bias\")] = f\"{mean_ate_bias:.3f} ± {std_ate_bias:.3f}\" if not pd.isna(mean_ate_bias) else np.nan\n",
    "                row[(base_model, \"runtime [s]\")] = round(runtime) if not pd.isna(runtime) else np.nan\n",
    "\n",
    "            records.append(row)\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    df.columns = pd.MultiIndex.from_tuples(df.columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Learner Results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>setup_name</th>\n",
       "      <th>scenario_key</th>\n",
       "      <th colspan=\"5\" halign=\"left\">ridge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CATE_MSE</th>\n",
       "      <th>ATE_pred</th>\n",
       "      <th>ATE_true</th>\n",
       "      <th>ATE_bias</th>\n",
       "      <th>runtime [s]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RCT_0_5</td>\n",
       "      <td>scenario_1</td>\n",
       "      <td>0.688 ± 0.068</td>\n",
       "      <td>0.099 ± 0.047</td>\n",
       "      <td>0.126 ± 0.011</td>\n",
       "      <td>-0.027 ± 0.050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  setup_name scenario_key          ridge                                \\\n",
       "                                CATE_MSE       ATE_pred       ATE_true   \n",
       "0    RCT_0_5   scenario_1  0.688 ± 0.068  0.099 ± 0.047  0.126 ± 0.011   \n",
       "\n",
       "                               \n",
       "         ATE_bias runtime [s]  \n",
       "0  -0.027 ± 0.050           1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"T-Learner Results\")\n",
    "summary_df = summarize_experiment_results(results_dict)\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_survival_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
